{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doc2Vec Train and Test\n",
    "The idea is to ultimately create a module that takes the data frame and return, instead of the body text, retuns a vector for each paragraph input (data input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate and Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train and test data\n",
    "\n",
    "* Data (train and test): [Cleaned reddit dataset](../../data/ad_hominem/ad_hominems_cleaned.csv), the data will be separated into test and train in a 70-30 ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set file names for train and test data\n",
    "data = pd.read_csv(\"../../data/ad_hominem/ad_hominems_cleaned.csv\")\n",
    "train_data, test_data = np.split(data.sample(frac=1), [int(.7*len(data))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Function to Read and Preprocess Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Define a function to open the train/test file (with latin encoding)\n",
    "* Read the file line-by-line\n",
    "* Pre-process each line using a simple gensim pre-processing tool (i.e., tokenize text into individual words, remove punctuation, set to lowercase, etc)\n",
    "* Return a list of words.\n",
    "Note that, for the data frame (corpus), each row constitutes a single document and the length of row entry (i.e., document) can vary. Also, to train the model, we'll need to associate a tag/number with each document of the training corpus. In our case, the tag is simply the index for the data frame (row number)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(df, tokens_only=False):\n",
    "    #with smart_open.smart_open(fname, encoding=\"iso-8859-1\") as f:\n",
    "    for i, line in df.iterrows():\n",
    "        if tokens_only:\n",
    "            yield gensim.utils.simple_preprocess(str(line[\"reddit_ad_hominem.body\"]))\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(gensim.utils.simple_preprocess(str(line[\"reddit_ad_hominem.body\"])), [i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(read_corpus(train_data))\n",
    "test_corpus = list(read_corpus(test_data, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the training corpus (both in the data frame and the generated corpus to see the differences):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_ad_hominem.body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31528</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7202</th>\n",
       "      <td>Nobody can force you to not contradict yourself, I guess. If you want to hold onto the social contract while not accepting the conclusions that follow from it, you are of course free to be irrational and undermine your own position on morality.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                     reddit_ad_hominem.body\n",
       "31528  NaN                                                                                                                                                                                                                                                 \n",
       "7202   Nobody can force you to not contradict yourself, I guess. If you want to hold onto the social contract while not accepting the conclusions that follow from it, you are of course free to be irrational and undermine your own position on morality."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "pd.DataFrame(train_data.loc[:, \"reddit_ad_hominem.body\"])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words=['nan'], tags=[31528]),\n",
       " TaggedDocument(words=['nobody', 'can', 'force', 'you', 'to', 'not', 'contradict', 'yourself', 'guess', 'if', 'you', 'want', 'to', 'hold', 'onto', 'the', 'social', 'contract', 'while', 'not', 'accepting', 'the', 'conclusions', 'that', 'follow', 'from', 'it', 'you', 'are', 'of', 'course', 'free', 'to', 'be', 'irrational', 'and', 'undermine', 'your', 'own', 'position', 'on', 'morality'], tags=[7202])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_corpus[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the testing corpus looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_ad_hominem.body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41508</th>\n",
       "      <td>no nothing. What you are seeing is a lot of individuals</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36957</th>\n",
       "      <td>I also understand that for other cultures</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         reddit_ad_hominem.body\n",
       "41508   no nothing. What you are seeing is a lot of individuals\n",
       "36957   I also understand that for other cultures              "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(test_data.loc[:, \"reddit_ad_hominem.body\"])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['no', 'nothing', 'what', 'you', 'are', 'seeing', 'is', 'lot', 'of', 'individuals'], ['also', 'understand', 'that', 'for', 'other', 'cultures']]\n"
     ]
    }
   ],
   "source": [
    "print(test_corpus[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the testing corpus is just a list of lists and does not contain any tags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "### Instantiate a Doc2Vec Object \n",
    "Doc2Vec model with:\n",
    "* Vector size with 500 words\n",
    "* Iterating over the training corpus 10 times (More iterations take more time and eventually reach a point of diminishing returns)\n",
    "* Minimum word count set to 20 (discard words with very few occurrences)\n",
    "\n",
    "Note: retaining infrequent words can often make a model worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "  model = gensim.models.doc2vec.Doc2Vec(vector_size=500, min_count=10, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, the vocabulary is a dictionary (accessible via `model.wv.vocab`) of all of the unique words extracted from the training corpus along with the count (e.g., `model.wv.vocab['penalty'].count` for counts for the word `penalty`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 29s, sys: 4.51 s, total: 3min 33s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "%time model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inferring a Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This vector can then be compared with other vectors via cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01656617, -0.01430459, -0.05245181, -0.00066138,  0.00586664,\n",
       "        0.01258109, -0.00204146, -0.02078112,  0.01407013, -0.01110554,\n",
       "       -0.01981455,  0.02212978,  0.02192996,  0.02683485, -0.00126907,\n",
       "        0.0400483 , -0.02251391,  0.01554183, -0.01737957,  0.05833966,\n",
       "       -0.0238959 ,  0.06652302, -0.01765297,  0.01790353,  0.01940756,\n",
       "       -0.02121411,  0.00521518, -0.01025048,  0.02276885,  0.04519996,\n",
       "        0.01079008,  0.01254493,  0.01227546,  0.02644348,  0.03949171,\n",
       "        0.0028408 , -0.02339171, -0.00848808,  0.01235549, -0.00383482,\n",
       "        0.02303251,  0.02827961, -0.03533961, -0.04026923, -0.00180538,\n",
       "        0.03984473,  0.05694033,  0.02689684,  0.01986732,  0.01301446,\n",
       "       -0.01341978,  0.05513496,  0.00897647, -0.03526007,  0.03911593,\n",
       "       -0.01551447, -0.02599545, -0.03524369,  0.03231908,  0.0451032 ,\n",
       "        0.04861251,  0.03795578, -0.00419231,  0.00696138, -0.00077949,\n",
       "       -0.03132649, -0.04002546, -0.04368919,  0.02382658, -0.01144554,\n",
       "        0.08567266, -0.00141577, -0.03072687, -0.0441585 ,  0.04445791,\n",
       "        0.00623465,  0.05114996, -0.00449917,  0.01678184,  0.01838384,\n",
       "       -0.01726673, -0.04462708, -0.05025112,  0.04808305,  0.00427837,\n",
       "        0.00727297, -0.01822091, -0.05167887, -0.01643765,  0.00374862,\n",
       "       -0.02346771, -0.00264895, -0.0029589 , -0.00471493, -0.04285786,\n",
       "        0.01420855, -0.03731376, -0.0024135 ,  0.03307418,  0.02725638,\n",
       "       -0.01460708, -0.02454722,  0.03556192, -0.00591952,  0.02895962,\n",
       "       -0.00450216, -0.03748104, -0.00652542,  0.0078406 ,  0.00289299,\n",
       "        0.01677114, -0.00377759, -0.03354688,  0.00939227, -0.03042607,\n",
       "       -0.00385014,  0.03459964, -0.06109586, -0.04792918,  0.00410046,\n",
       "        0.03645805,  0.04079868,  0.0393347 ,  0.02339115,  0.00986069,\n",
       "       -0.03541368, -0.05874   , -0.00928361,  0.00700203,  0.00397462,\n",
       "        0.00290005,  0.00950719, -0.01685845,  0.01093557,  0.01668863,\n",
       "        0.02720606, -0.01781745, -0.02491938,  0.02278412, -0.0313805 ,\n",
       "        0.0164014 ,  0.05992708,  0.03949498, -0.04230603,  0.02780678,\n",
       "        0.04414213,  0.02993848, -0.00971266,  0.04339151,  0.00480061,\n",
       "        0.04276416, -0.02688152,  0.01497222,  0.02817392, -0.02965821,\n",
       "       -0.02797297,  0.00534771, -0.05108379,  0.00678908, -0.04561217,\n",
       "       -0.00375104, -0.02558918, -0.0343252 , -0.00985129,  0.00682128,\n",
       "       -0.00768139, -0.01295578, -0.04356981,  0.03055245,  0.00970002,\n",
       "       -0.00186677,  0.02058962, -0.01410752, -0.02806109, -0.03073141,\n",
       "       -0.00883332,  0.00064229, -0.02145182, -0.04021379,  0.01619555,\n",
       "       -0.01753824, -0.0262987 , -0.0416689 , -0.06546678, -0.029225  ,\n",
       "        0.00771241,  0.02922441, -0.02084332,  0.00629893,  0.04604667,\n",
       "        0.01682107,  0.03335942, -0.01640845,  0.02413216, -0.00127522,\n",
       "       -0.01918612, -0.03872893, -0.01897497,  0.01545167, -0.08123791,\n",
       "       -0.0418113 , -0.00710926, -0.06063582,  0.03433117, -0.00193335,\n",
       "        0.02462521,  0.02681641,  0.00221435,  0.00152807, -0.01400469,\n",
       "        0.00258174, -0.02774789,  0.00031154,  0.03208718, -0.02941457,\n",
       "        0.00737739, -0.00968925, -0.0043753 , -0.04884771,  0.01703841,\n",
       "        0.01923571,  0.01710414, -0.03560843, -0.02984017,  0.01683461,\n",
       "       -0.03129362, -0.00877789,  0.02007621, -0.01487427, -0.00817407,\n",
       "       -0.01926328,  0.05943997, -0.02101068,  0.0353136 ,  0.03547206,\n",
       "        0.01045104,  0.02395212,  0.02809737, -0.00546635,  0.0048484 ,\n",
       "        0.02349214,  0.02921702, -0.00057388, -0.03430847, -0.01506232,\n",
       "        0.02556592,  0.02163438, -0.00362727,  0.00476407, -0.01940857,\n",
       "       -0.01845073, -0.01566713,  0.06925838,  0.03849254, -0.00723688,\n",
       "        0.05059091, -0.00542786, -0.06383346,  0.010153  ,  0.05200597,\n",
       "       -0.03456296, -0.02544682,  0.00190453, -0.02919926,  0.02550345,\n",
       "       -0.01349382, -0.02037719, -0.030105  , -0.00348406,  0.03156903,\n",
       "       -0.02669406,  0.00623001, -0.02100731, -0.01220511,  0.03790618,\n",
       "        0.01032814, -0.04983278, -0.02250327, -0.0068184 , -0.01508471,\n",
       "       -0.03343642, -0.01315095, -0.00284083,  0.05680535,  0.03275793,\n",
       "       -0.01741126,  0.05522197, -0.02405252,  0.00617022,  0.03917235,\n",
       "       -0.01030789, -0.011691  , -0.03502734, -0.01384081, -0.058674  ,\n",
       "       -0.00246295,  0.00172388,  0.00251637,  0.00184555, -0.04310085,\n",
       "       -0.00340151,  0.01342298, -0.02997612, -0.02986569, -0.03503497,\n",
       "        0.00044946,  0.04712398, -0.01026301,  0.01348951,  0.00489265,\n",
       "        0.00862762, -0.03579537, -0.01160638, -0.01424116,  0.04558476,\n",
       "       -0.01172316,  0.03182713, -0.01323745,  0.02905478, -0.01405651,\n",
       "        0.02412766, -0.04765615,  0.01099168, -0.00562737,  0.01920343,\n",
       "       -0.03847275, -0.0484436 ,  0.03036922,  0.01987275, -0.02549072,\n",
       "       -0.01908487, -0.05708521,  0.01347778, -0.03524161, -0.04767616,\n",
       "        0.01330998,  0.00506959, -0.02464597, -0.00109552,  0.02766246,\n",
       "       -0.00599615,  0.02234245, -0.02224259,  0.057591  ,  0.00976465,\n",
       "       -0.02962353,  0.06200285,  0.02475918,  0.03543429, -0.00748558,\n",
       "        0.01581526, -0.01135247, -0.03283836, -0.01268176,  0.02016878,\n",
       "       -0.03689624,  0.02583529, -0.00145463, -0.01311926, -0.00241693,\n",
       "       -0.03401728, -0.01219461,  0.04744369, -0.03499468,  0.04325366,\n",
       "        0.02377408,  0.01909221,  0.02058322,  0.00155128,  0.02995543,\n",
       "       -0.0350811 ,  0.04298503, -0.0179756 ,  0.00093768, -0.01035755,\n",
       "       -0.01458453,  0.02000397, -0.0589852 ,  0.0077045 , -0.00107891,\n",
       "       -0.00970003, -0.01453179,  0.01044502, -0.00250772,  0.00277481,\n",
       "        0.01664465, -0.00253691, -0.00583208, -0.00483057,  0.02092412,\n",
       "        0.02084542,  0.01927697, -0.0011545 , -0.00787895, -0.00418811,\n",
       "       -0.05145828, -0.03738078, -0.01658273, -0.02339991,  0.06401184,\n",
       "       -0.04387458, -0.02260544,  0.03970537, -0.01947934, -0.04963639,\n",
       "       -0.01149573, -0.03641446,  0.00015082, -0.00842556, -0.00235199,\n",
       "        0.04294765, -0.00511035, -0.03544492, -0.07271367, -0.01949687,\n",
       "        0.00976092, -0.03759611, -0.04071638, -0.02016885,  0.03050849,\n",
       "       -0.04067411, -0.03416151, -0.04140911,  0.0408347 , -0.03378656,\n",
       "       -0.00439127, -0.01674843,  0.03172919,  0.01488382,  0.01975298,\n",
       "        0.00590985,  0.00220117,  0.00927284, -0.00689055,  0.03071417,\n",
       "       -0.03478868,  0.02285984, -0.02709965, -0.0317629 ,  0.01235508,\n",
       "       -0.02764394,  0.02029487, -0.03212184, -0.01265421, -0.03349945,\n",
       "        0.02513069, -0.02917806, -0.02014887, -0.03731871, -0.04504375,\n",
       "        0.06121881,  0.0029123 , -0.02854002,  0.05708912,  0.01184721,\n",
       "        0.02004679,  0.01408039,  0.05701686, -0.01075948,  0.00745241,\n",
       "       -0.02422787, -0.00979144,  0.0150567 , -0.00096038, -0.03628506,\n",
       "       -0.0050095 ,  0.04744714, -0.03168337, -0.01163862,  0.02630838,\n",
       "        0.02696764,  0.02686594,  0.01005644, -0.01878385,  0.02218107,\n",
       "       -0.03774583,  0.00301562,  0.02970547, -0.0167516 ,  0.0302779 ,\n",
       "        0.04184054,  0.04633318, -0.01551477, -0.00199681,  0.02591555,\n",
       "       -0.01486709,  0.02737186,  0.01087992,  0.03943464,  0.01268444,\n",
       "        0.03825281, -0.00418349, -0.04822833,  0.03806787, -0.03130038,\n",
       "       -0.01587752,  0.02107986, -0.02089495,  0.02562381,  0.02544067],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['only', 'you', 'can', 'prevent', 'forest', 'fires'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `infer_vector()` takes a list of *string tokens*\n",
    "* Input should be tokenized prior to inference\n",
    "    * Here the test set is already tokenized (in `test_corpus = list(read_corpus(test_data, tokens_only=True))`)\n",
    "    \n",
    "Note: algorithms use internal randomization, so repeated inferences of the same text will return slightly different vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the model\n",
    "\n",
    "Deleting training data from memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"reddit-doc2vec.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To load the model:\n",
    "\n",
    "`model = Doc2Vec.load(fname)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To use model for inference:\n",
    "\n",
    "`vector = model.infer_vector([\"tokenized\", \"input\", \"string\"])`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "### To load the model:\n",
    "* `model = Doc2Vec.load(fname)` (not required here)\n",
    "### To use model for inference:\n",
    "* `vector = model.infer_vector([\"tokenized\", \"input\", \"string\"])`\n",
    "    #### To tokenize:\n",
    "    * `list(read_corpus(df, tokens_only=False))` (used earlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized test sample:\n",
      "['also', 'understand', 'that', 'for', 'other', 'cultures']\n",
      "\n",
      "Inferred vector:\n",
      "[-3.50757921e-03  1.90081634e-03 -3.46079506e-02  3.77507391e-03\n",
      "  1.34055242e-02 -7.74921477e-03 -2.19890289e-02 -3.07108499e-02\n",
      "  3.56810503e-02  5.54005615e-02 -4.47838940e-02  3.77739333e-02\n",
      " -2.31937189e-02  1.90163720e-02 -2.43136268e-02  9.67435092e-02\n",
      "  3.04232612e-02  1.01761976e-02  1.53096411e-02  8.95579010e-02\n",
      " -2.06847843e-02  7.23380819e-02 -3.11913360e-02  4.82664816e-02\n",
      "  3.21813077e-02 -4.79818974e-03  3.61051410e-02 -4.30038273e-02\n",
      "  1.44684920e-02  5.99387325e-02  6.93687983e-03  5.27522415e-02\n",
      " -3.82570201e-03 -1.77666359e-02  5.55941910e-02  8.42079613e-03\n",
      " -3.17627452e-02  2.78663840e-02 -2.17901040e-02 -2.85427496e-02\n",
      "  5.90239326e-03  3.40274535e-02 -1.83005985e-02 -5.19166663e-02\n",
      "  1.77615825e-02  1.83763709e-02  4.02364284e-02  3.56108323e-02\n",
      "  8.76685418e-03  3.27675901e-02 -2.77509168e-02  5.70554771e-02\n",
      "  4.32702154e-03 -4.53536026e-02  6.18866310e-02 -1.55778043e-03\n",
      " -4.25656587e-02 -3.65616865e-02  1.06312102e-02  1.49347233e-02\n",
      " -2.06097327e-02 -1.54770315e-02 -1.11146794e-04  1.96899120e-02\n",
      " -3.31733585e-03 -4.61247601e-02 -4.28170897e-02 -6.30276799e-02\n",
      "  2.55074985e-02 -3.26518691e-03  6.54885620e-02 -3.41236442e-02\n",
      " -3.25117856e-02 -4.86826450e-02  4.12136456e-03  8.60307273e-03\n",
      "  8.62591267e-02  1.58616342e-02  2.57700123e-02  2.52628624e-02\n",
      " -3.16694789e-02 -3.49785313e-02 -4.55212928e-02  2.99569052e-02\n",
      "  1.03874095e-02  4.14391048e-02 -4.08719070e-02 -3.26691046e-02\n",
      " -2.40018703e-02 -4.59064692e-02 -5.34132905e-02 -3.34163792e-02\n",
      " -6.45729806e-03 -4.22847457e-02 -7.98544474e-03  1.29715689e-02\n",
      "  2.02293904e-03  1.70661565e-02  1.04015842e-02  2.26562340e-02\n",
      "  4.14282922e-03 -8.05861503e-02  3.21671180e-02  7.65525782e-03\n",
      " -2.92127673e-02  7.39187468e-03 -3.48809958e-02 -1.14260549e-02\n",
      "  3.43632139e-02 -4.12283186e-03 -1.99483568e-03  2.30032224e-02\n",
      " -4.58565541e-02 -3.21141742e-02 -1.82727154e-03 -4.55590263e-02\n",
      " -9.92861111e-03 -6.32445514e-02 -2.46361122e-02  1.64818354e-02\n",
      " -1.25113133e-04 -2.64221318e-02  2.90135741e-02  3.00646666e-02\n",
      "  2.05495469e-02 -2.16197353e-02 -1.04752574e-02 -4.31850813e-02\n",
      "  8.19629431e-03  2.94193905e-02 -1.42699003e-03  2.49525369e-03\n",
      " -1.02445940e-02  3.51690054e-02 -8.21865816e-03  2.87375227e-02\n",
      "  2.01747194e-03 -3.36853340e-02  1.92657486e-02 -6.49854988e-02\n",
      "  8.20379425e-03  3.25378627e-02  1.25941215e-02 -2.12187283e-02\n",
      "  1.75567959e-02  3.26902084e-02  1.10726096e-02 -3.66319865e-02\n",
      "  3.46985087e-02  2.01078355e-02  2.05339100e-02 -1.21380584e-02\n",
      "  2.61108321e-03 -1.74030960e-02 -5.86022809e-02  2.46849079e-02\n",
      "  2.99351173e-03 -3.45820226e-02  4.71528433e-03 -3.05184200e-02\n",
      " -3.29002179e-02  2.11551432e-02  1.92219170e-03 -1.49263730e-02\n",
      " -1.71376094e-02 -8.24270770e-03  3.00534628e-02 -3.22917774e-02\n",
      "  5.80383353e-02  1.97622534e-02 -1.17712384e-02 -2.91009113e-04\n",
      "  2.72486359e-03 -1.47134466e-02 -2.08821241e-02  2.34537628e-02\n",
      "  3.01168654e-02 -6.28369395e-03 -3.96017879e-02  5.83281890e-02\n",
      " -3.50581929e-02 -6.51158765e-02 -2.25906074e-02 -5.36040477e-02\n",
      "  1.03390189e-02  4.21327986e-02  4.80708852e-02  6.55367831e-03\n",
      " -3.20368037e-02  6.82566687e-02  2.18016412e-02  3.36723290e-02\n",
      " -1.65861361e-02 -1.01643223e-02 -1.15577290e-02 -4.89189290e-02\n",
      "  7.05488492e-03 -7.55431969e-03  4.75171693e-02 -8.00282806e-02\n",
      " -4.23744097e-02  2.87506916e-02 -1.00551182e-02  4.67178635e-02\n",
      " -5.93067892e-03  2.69014854e-02  3.71370614e-02 -9.15082358e-03\n",
      "  2.21365895e-02 -4.25744355e-02  3.93893570e-03 -4.71402146e-03\n",
      "  2.46540830e-02  4.25527394e-02 -3.74969356e-02 -2.22242251e-02\n",
      " -4.38210815e-02 -1.46180950e-02 -5.94167337e-02  3.46799158e-02\n",
      "  3.71171497e-02  2.49722973e-02 -2.58269869e-02  1.65775102e-02\n",
      "  1.44935204e-02 -3.90914381e-02 -1.43023711e-02 -1.17035843e-02\n",
      " -5.39636463e-02  1.55180003e-02 -3.97463851e-02  9.07720402e-02\n",
      " -1.76137276e-02  6.01768494e-02 -5.09924255e-03  2.40061264e-02\n",
      "  1.15014762e-02  1.32210674e-02 -2.77252253e-02  3.71933468e-02\n",
      "  9.57299862e-03  4.39159423e-02 -3.84500548e-02 -3.58026586e-02\n",
      " -1.52310636e-02  1.32804709e-02 -3.60657927e-04  1.18057372e-03\n",
      "  1.91382691e-02  1.39688952e-02 -3.51635329e-02 -1.51160499e-02\n",
      "  7.95298144e-02  1.77458189e-02  7.56435795e-03  6.34566769e-02\n",
      " -2.74324771e-02 -4.79067527e-02  2.35418584e-02  7.18988329e-02\n",
      " -1.82119850e-02  1.42554846e-02 -2.45361519e-03 -5.21800369e-02\n",
      "  2.78778039e-02  1.76521833e-03  2.69318409e-02 -2.37924345e-02\n",
      "  7.93549046e-03  2.37624068e-02 -4.42949124e-02  4.21646197e-04\n",
      " -6.06833817e-03  3.82049684e-03  2.45121550e-02  1.31484540e-03\n",
      " -3.25217210e-02 -1.39899796e-03 -1.79979466e-02 -3.06787901e-02\n",
      " -6.17211312e-02  7.19039515e-03 -2.57974733e-02  7.71527290e-02\n",
      "  5.79148438e-03 -1.99064873e-02  8.75614956e-02 -1.74093638e-02\n",
      "  7.55602345e-02  3.17277131e-03  1.01146726e-02 -4.81329635e-02\n",
      " -4.47601154e-02  3.00877094e-02 -2.59260386e-02 -6.02877815e-04\n",
      " -1.37758220e-03 -4.37216554e-03  7.24420417e-03 -2.33410690e-02\n",
      "  1.29614724e-02 -2.93315314e-02 -5.92772886e-02 -9.35089029e-03\n",
      " -3.63294668e-02  1.48520088e-02  4.20757495e-02 -3.84387076e-02\n",
      "  9.82116559e-04  2.95895655e-02  4.32541743e-02 -3.87972929e-02\n",
      " -9.10595898e-03 -2.31928332e-03  1.16133690e-02  2.84787873e-03\n",
      " -2.60164472e-03 -1.80150289e-02  2.40662065e-03 -2.09883749e-02\n",
      "  1.95297487e-02 -5.52493595e-02  1.40148001e-02 -6.78013079e-03\n",
      "  2.41058581e-02 -3.38383541e-02 -5.82411289e-02 -5.54397376e-03\n",
      "  3.34433652e-02 -1.22118443e-02 -3.72367390e-02 -7.09867626e-02\n",
      " -4.89472924e-03 -2.06942689e-02 -8.11797082e-02 -1.55763431e-02\n",
      " -8.53668340e-03  6.48579618e-04 -3.59608512e-03  2.31416654e-02\n",
      " -1.27657019e-02  3.02928928e-02 -4.31395546e-02  4.44801487e-02\n",
      " -7.81558361e-03 -3.98119204e-02  1.84460245e-02 -1.30948108e-02\n",
      "  5.82912602e-02  5.01553118e-02 -2.96061859e-03  1.68325175e-02\n",
      " -2.55140644e-02 -6.58143498e-03  1.23506496e-02 -1.70612670e-02\n",
      "  2.96967383e-02 -4.41227555e-02  2.90428943e-05  3.02832760e-02\n",
      " -4.49607894e-02 -4.28160727e-02  6.85138777e-02 -4.45566550e-02\n",
      "  3.42859514e-02  7.20406044e-03  1.47466725e-02  1.05373142e-02\n",
      "  2.83430493e-03  6.83141407e-03 -9.00288671e-03  2.90060993e-02\n",
      " -3.96390483e-02  3.13742980e-02 -1.63426660e-02 -5.69583848e-02\n",
      " -1.18878447e-02 -4.99696061e-02 -3.94915529e-02 -2.77926382e-02\n",
      " -2.43104603e-02 -1.86380162e-03  2.11938433e-02  2.09139530e-02\n",
      "  2.52545904e-03  3.39879096e-02 -8.78804014e-04 -4.48150858e-02\n",
      "  3.74770584e-03  2.46468186e-02  4.47367169e-02 -9.44544002e-03\n",
      " -2.00756323e-02  1.20568005e-02  1.64608918e-02 -1.20666465e-02\n",
      " -3.96107994e-02 -4.27440777e-02 -2.53859926e-02  6.17599450e-02\n",
      " -2.81821396e-02 -1.66727863e-02  1.54594649e-02 -2.58684270e-02\n",
      " -3.79755236e-02 -2.85728723e-02 -8.68294314e-02 -2.29430269e-03\n",
      "  1.30644126e-03 -2.53623035e-02  1.68453641e-02 -1.58287734e-02\n",
      " -3.66712105e-03 -7.13011548e-02 -3.41442018e-03  2.84308679e-02\n",
      " -1.07932296e-02 -4.85458933e-02  1.58031769e-02  5.21682724e-02\n",
      " -1.65378843e-02 -2.53729504e-02 -4.64364029e-02  5.80839999e-02\n",
      " -5.69612421e-02 -3.04044392e-02 -1.02954661e-03 -2.13146023e-02\n",
      "  2.16828994e-02  1.03778262e-02 -3.07227168e-02 -9.50275827e-03\n",
      "  3.66376415e-02 -3.61624807e-02  1.82460826e-02 -5.87487668e-02\n",
      " -1.19660413e-02 -1.97490677e-02  6.06471847e-04  3.32808234e-02\n",
      " -2.59910543e-02  6.37860000e-02 -3.76084410e-02 -2.54789740e-03\n",
      " -2.01912057e-02  3.90787460e-02 -2.50402931e-02 -3.27229425e-02\n",
      " -4.63524461e-02 -4.39914316e-02  1.07528195e-01 -2.00333353e-02\n",
      " -3.22125554e-02  5.91186099e-02  1.71979722e-02  1.58869810e-02\n",
      "  4.08376269e-02  6.93080425e-02 -4.15751385e-03  2.62688799e-03\n",
      "  3.13203260e-02 -2.03311052e-02  4.48752893e-03 -8.40330776e-03\n",
      " -1.16003128e-02 -8.65347497e-03  3.04197241e-02 -4.25790884e-02\n",
      " -1.61974635e-02  3.73723395e-02  2.39411741e-02  4.86944728e-02\n",
      "  1.31892189e-02  1.56820044e-02 -9.50342583e-05 -7.12608546e-02\n",
      "  2.10593920e-02 -9.66759771e-03 -6.66681379e-02  1.33477792e-03\n",
      " -4.50883759e-03  1.70116108e-02 -1.62506104e-02 -3.61667536e-02\n",
      "  2.03652922e-02  2.86104716e-03  3.08741853e-02  6.30165217e-03\n",
      "  4.62424681e-02  4.88988273e-02  4.56286445e-02 -3.23024467e-02\n",
      " -2.17440315e-02  6.72615021e-02 -2.08439268e-02 -4.24159020e-02\n",
      "  4.33635451e-02 -2.29659323e-02  5.51529080e-02  3.76619101e-02]\n"
     ]
    }
   ],
   "source": [
    "vector_sample = model.infer_vector(test_corpus[1])\n",
    "print(\"Tokenized test sample:\")\n",
    "print(test_corpus[1])\n",
    "print(\"\\nInferred vector:\")\n",
    "print(vector_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For more:\n",
    "* [Yaron Vazana](http://yaronvazana.com/2018/01/20/training-doc2vec-model-with-gensim/)\n",
    "* [Rare Technologies](https://rare-technologies.com/doc2vec-tutorial/)\n",
    "* [Gensim Documentation](https://radimrehurek.com/gensim/models/doc2vec.html)\n",
    "* [Gensim Doc2Vec Tutorial on the IMDB Sentiment Dataset](https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb)\n",
    "* [Doc2Vec Tutorial on the Lee Dataset](https://markroxor.github.io/gensim/static/notebooks/doc2vec-lee.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
