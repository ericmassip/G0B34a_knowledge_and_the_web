{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.svm import LinearSVC, NuSVC, OneClassSVM, SVC, SVR, l1_min_c\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import sys\n",
    "sys.path.insert(0, '/home/mcunha/Documents/Classes/KW/G0B34a_knowledge_and_the_web/')\n",
    "import data.ad_hominem.tokenize_df\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining a function that will be used later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just cleaned the data frame a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reddit_ad_hominem.body</th>\n",
       "      <th>reddit_ad_hominem.ad_hominem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29277</th>\n",
       "      <td>we re gonna back him because we won know the d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29278</th>\n",
       "      <td>gender</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29279</th>\n",
       "      <td>which was alienating and confusing it makes yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29280</th>\n",
       "      <td>and the such my relationship was my baptize by...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29281</th>\n",
       "      <td>because that was label that other can understand</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29282</th>\n",
       "      <td>it is more effective to report it than downvot...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29283</th>\n",
       "      <td>which was alienating and confusing it makes yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29284</th>\n",
       "      <td>because that was quot label that other</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29285</th>\n",
       "      <td>times more iron fragments than asbestos</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29286</th>\n",
       "      <td>ve never heard of supreme court rulings allowi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  reddit_ad_hominem.body  \\\n",
       "29277  we re gonna back him because we won know the d...   \n",
       "29278                                             gender   \n",
       "29279  which was alienating and confusing it makes yo...   \n",
       "29280  and the such my relationship was my baptize by...   \n",
       "29281   because that was label that other can understand   \n",
       "29282  it is more effective to report it than downvot...   \n",
       "29283  which was alienating and confusing it makes yo...   \n",
       "29284             because that was quot label that other   \n",
       "29285            times more iron fragments than asbestos   \n",
       "29286  ve never heard of supreme court rulings allowi...   \n",
       "\n",
       "       reddit_ad_hominem.ad_hominem  \n",
       "29277                             0  \n",
       "29278                             0  \n",
       "29279                             0  \n",
       "29280                             0  \n",
       "29281                             0  \n",
       "29282                             0  \n",
       "29283                             0  \n",
       "29284                             0  \n",
       "29285                             0  \n",
       "29286                             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallacies = pd.read_csv(\"../../data/ad_hominem/ad_hominems_cleaned_Murilo.csv\")\n",
    "fallacies = fallacies.drop(['Unnamed: 0'], axis=1)\n",
    "fallacies = data.ad_hominem.tokenize_df.preprocess_df(fallacies)\n",
    "fallacies['reddit_ad_hominem.body'].replace('', np.nan, inplace=True)\n",
    "fallacies.dropna(subset=['reddit_ad_hominem.body'], inplace=True)\n",
    "fallacies.reset_index()\n",
    "train_data, test_data = train_test_split(fallacies, test_size=0.3, random_state=3)\n",
    "\n",
    "fallacies.tail(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I used the doc2vec from [here](/models/02_doc2vec/doc2vec.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading doc2vec model...\n",
      "Done!\n",
      "Preparing the train data...\n",
      "Done!\n",
      "Preparing the test data...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading doc2vec model...\")\n",
    "model = gensim.models.doc2vec.Doc2Vec.load(\"reddit-doc2vec.model\")\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Preparing the train data...\")\n",
    "x_train = [gensim.utils.simple_preprocess(i) for i in train_data[\"reddit_ad_hominem.body\"]] # Tokenize and remove stop words, make lower case, etc.\n",
    "x_train = [model.infer_vector(i) for i in x_train]                                          # Infer vectors\n",
    "y_train = list(train_data[\"reddit_ad_hominem.ad_hominem\"])\n",
    "print(\"Done!\")\n",
    "\n",
    "print(\"Preparing the test data...\")\n",
    "x_test = [gensim.utils.simple_preprocess(i) for i in test_data[\"reddit_ad_hominem.body\"]] # Tokenize and remove stop words, make lower case, etc.\n",
    "x_test = [model.infer_vector(i) for i in x_test]                                          # Infer vectors\n",
    "y_test = list(test_data[\"reddit_ad_hominem.ad_hominem\"])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras import utils\n",
    "from keras.preprocessing import text, sequence\n",
    "\n",
    "vocab_size = 100000\n",
    "\n",
    "tokenize = text.Tokenizer(num_words=vocab_size)\n",
    "#tokenize.fit_on_texts(result.headline_text)\n",
    "\n",
    "tokenize.fit_on_texts(df[\"body\"]) # only fit on train\n",
    "x_train = tokenize.texts_to_matrix(train[\"body\"])\n",
    "x_test = tokenize.texts_to_matrix(test[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reddit_ad_hominem.body          object\n",
       "reddit_ad_hominem.ad_hominem     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fallacies.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20463,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Conv1D\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1024, input_shape=(500,))) # 500 is from the doc2vec model\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1024)              513024    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 775,681\n",
      "Trainable params: 775,681\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 18416 samples, validate on 2047 samples\n",
      "Epoch 1/4\n",
      "18416/18416 [==============================] - 4s 200us/step - loss: 14.5590 - acc: 0.0868 - val_loss: 14.3458 - val_acc: 0.1001\n",
      "Epoch 2/4\n",
      "18416/18416 [==============================] - 3s 138us/step - loss: 14.5590 - acc: 0.0868 - val_loss: 14.3458 - val_acc: 0.1001\n",
      "Epoch 3/4\n",
      "18416/18416 [==============================] - 2s 131us/step - loss: 14.5590 - acc: 0.0868 - val_loss: 14.3458 - val_acc: 0.1001\n",
      "Epoch 4/4\n",
      "18416/18416 [==============================] - 2s 130us/step - loss: 14.5590 - acc: 0.0868 - val_loss: 14.3458 - val_acc: 0.1001\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
